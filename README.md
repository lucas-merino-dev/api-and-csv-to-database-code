# ğŸ“Š Data Pipeline â€“ APIs + CSV â†’ MySQL â†’ Power BI (WIP)

Este projeto tem como objetivo **capturar, processar e armazenar dados provenientes de mÃºltiplas APIs e arquivos CSV**, inserindo-os em um banco **MySQL**, para posterior consumo e modelagem no **Power BI**.

O sistema automatiza a coleta, padronizaÃ§Ã£o e persistÃªncia dos dados, garantindo atualizaÃ§Ã£o consistente para anÃ¡lises e dashboards.

---

## ğŸš€ Funcionalidades

- ğŸ”„ **Coleta automÃ¡tica** de dados em APIs de sistemas gerenciais.  
- ğŸ“ **Leitura e tratamento de arquivos CSV**.  
- ğŸ§¹ PadronizaÃ§Ã£o de campos (datas, nÃºmeros, mensagens, status, chaves, IDs etc.).  
- ğŸ—„ï¸ InserÃ§Ã£o estruturada em tabelas MySQL.  
- ğŸ›¡ï¸ Tratamento de erros, logs e validaÃ§Ã£o de dados.  
- ğŸ“¡ IntegraÃ§Ã£o total com **Power BI** para relatÃ³rios e insights avanÃ§ados.  

---
## ğŸ“– Fluxo de Dados

1. **Coleta**  
   O script consulta APIs externas, utiliza tokens dinÃ¢micos, headers personalizados e trata paginaÃ§Ã£o e erros.
   
2. **TransformaÃ§Ã£o**  
   Dados brutos passam por:
   - NormalizaÃ§Ã£o de campos  
   - ConversÃ£o de tipos  
   - CorreÃ§Ã£o de formataÃ§Ã£o  
   - ValidaÃ§Ã£o de chaves  

3. **Carga (Load)**  
   Os dados sÃ£o inseridos no MySQL usando operaÃ§Ãµes otimizadas e controle de duplicidade.

4. **VisualizaÃ§Ã£o**  
   O Power BI consome as tabelas MySQL para criaÃ§Ã£o de mÃ©tricas, dashboards e anÃ¡lises.

---

## ğŸ› ï¸ Tecnologias Utilizadas

- **Python 3**
  - `requests` â€“ consumo de APIs  
  - `csv` / `pandas` â€“ leitura e manipulaÃ§Ã£o de CSV  
  - `mysql.connector` â€“ conexÃ£o com MySQL  
  - `logging` â€“ logs estruturados
  - `multiprocessing` - aceleraÃ§Ã£o de fluxo por multiprocessamento

- **MySQL 5.7/8+**

- **Power BI Desktop / Power BI Service**

- **Agendador de Tarefas Windows**
